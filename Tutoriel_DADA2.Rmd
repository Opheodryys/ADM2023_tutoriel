---
title: "R Notebook"
output: github_document
---
```{bash}
git config --global user.email "you@example.com"
git config --global user.name "Your Name"
```

1.

On commence par télécharger les fichiers avec la commande wget puis on extrait les fichiers.

```{bash, eval=FALSE}
wget https://github.com/ANF-MetaBioDiv/course-material/archive/refs/heads/main.zip
unzip main.zip
```

On commence par créer une variable refdb_folder qui contient le chemin d'accès vers un fichier. On demande à R de créer une direction vers la variable refdb_folder portant ce nom si jamais il n'existe pas. L'argument recursive = TRUE signifie qu'on autorise le fichier refdb à créer des sous-dossiers.

```{r}
refdb_folder <- here::here("data", "refdb")
refdb_folder
if (!dir.exists(refdb_folder)) dir.create(refdb_folder, recursive = TRUE)
```

On dit à R de copier les séquences dans le bon dossier.

```{bash}
cp -R course-material-main/data/raw ./data
```

On dit à R d'augmenter le temps avant qu'il arrête tout seul le téléchargement (on lui donne 20 minutes).

```{r}
getOption("timeout")
options(timeout=1200)
```

Cette variable reçoit le chemin dans le fichier refdb.

```{r}
silva_train_set <- file.path(refdb_folder, "silva_nr99_v138.1_train_set.fa.gz")

```

```{r}
silva_species_assignment <- file.path(refdb_folder,"silva_species_assignment_v138.1.fa.gz")
```

On télécharge ensuite les données sur Zenodo si le fichier de données n'existe pas dans le fichier. Pour la première, on télécharge ce fichier de données dans le bon fichier créé. L'argument quiet = TRUE permet de faire un DL sans afficher la progression du téléchargement. Pareil pour la deuxième.

```{r}
if (!file.exists(silva_train_set)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz",
    silva_train_set,
    quiet = TRUE
  )
}


if (!file.exists(silva_species_assignment)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz",
    silva_species_assignment,
    quiet = TRUE
  )
}


```

2.

On fait une nouvelle variable qui donnera le chemin aux données. On crée 2 nouvelles variables qui vont chacune regrouper les séquences possédant R1 dans l'une et R2 dans l'autre.
 
```{r}
path_to_fastqs <- here::here("data", "raw")

fnFs <- sort(list.files(path_to_fastqs, pattern = "_R1.fastq.gz", full.names = TRUE))

fnRs <- sort(list.files(path_to_fastqs, pattern = "_R2.fastq.gz",full.names = TRUE))
```

On extrait uniquement le nom de l'échantillon. basename retire le chemin pour y accéder pour garder uniquement le nom du fichier. |> permet de faire des chaînes de fonctions sans utiliser de commandes ou de parenthèses. La traduction est tuyau, donc la commande rentre dans le tuyau. strsplit() coupe la chaîne selon le pattern indiqué entre " ". sapply() applique une fonction à chaque élément de la liste ou du vecteur pour le simplifier, ici on sélectionne juste le premier élément de la liste (juste le nom des échantillons).

```{r}
sample_names <- basename(fnFs) |>
  strsplit(split = "_") |>
  sapply(head, 1)
```

D'abord, on liste les noms des fichiers R1.

```{r}
basename(fnFs) |>
  head()
```

La fonction strsplit() permet de séparer chaque nom de fichier à un endroit donné en un vecteur comportant 2 éléments. Le résultat est une liste de vecteurs avec 2 éléments.

```{r}
basename(fnFs) |>
  strsplit(split = "_") |>
  head()
```

Maintenant, on retient uniquement la première partie du nom, soit SxX.

```{r}
basename(fnFs) |>
  strsplit(split = "_") |>
  sapply(head, 1) |>
  head()
```

On indique à R l'endroit où doivent se trouver les outils.

```{r, eval=FALSE}
devtools::load_all(path="/home/rstudio/ADM2023_tutoriel/course-material-main/R")
```

3.

On crée une variable quality_folder avec le chemin menant aux graphiques. Ainsi, si le fichier n'existe pas, on le fait dans notre répertoire. Par exemple : fichier 1 : la séquence va de 1 à 300 pb. 2000 séquences du S11B sont lues. La ligne rouge est le nombre de read qui ont au moins cette longueur (ici 100% car Illumina fait obligatoirement la même longueur). Score de qualité diminue chez les R1 à la fin. On veut se débarrasser de tout ce qui est moins bien (garder tout en dessous de Q30). A partir de 200nt, on passe en dessous. Il faut toujours garder 20-30 nt de chevauchement pour les aligner. qualityprofile permet de vérifier la qualité des séquences brutes.

```{r}
quality_folder <- here::here("outputs",
                             "dada2",
                             "quality_plots")

if (!dir.exists(quality_folder)) {
  dir.create(quality_folder, recursive = TRUE)
}

qualityprofile(fnFs,
               fnRs,
               file.path(quality_folder, "quality_plots.pdf"))
```

4.

On souhaite raccourcir les séquences pour retirer les bases avec le moins bon Qscore. On crée une variable pour diriger R vers les séquences raccourcies. Pareil, si la direction n'existe pas, on crée une direction vers ce fichier.

```{r}
path_to_trimmed_reads <- here::here(
  "outputs",
  "dada2",
  "trimmed"
)

if (!dir.exists(path_to_trimmed_reads)) dir.create(path_to_trimmed_reads, recursive = TRUE)
```

On attribue à l'amorce directe et reverse leur séquence respective. 

```{r}
primer_fwd  <- "CCTACGGGNBGCASCAG"
primer_rev  <- "GACTACNVGGGTATCTAAT"
```

On lit les 10 premières (R1) et 10 dernières (R2) séquences en format fastq.

```{r}
Biostrings::readDNAStringSet(
  fnFs[1],
  format = "fastq",
  nrec = 10
)
```
```{r}
Biostrings::readDNAStringSet(
  fnRs[1],
  format = "fastq",
  nrec = 10
)
```

On copie les informations du dossier bash dans un nouveau dossier bash dans le ADM2023_tutoriel.

```{bash}
pwd
cp -R /home/rstudio/ADM2023_tutoriel/course-material-main/bash .
```

La variable primer_log prend la fonction primer_trim comme objet. On retire les amorces par cette fonction. forward et reverse correspondent aux séquences directes et complémentaires. primer_fwd et primer_rev correspondent aux amorces. output_dir permet de noter toutes les séquences raccourcies. Les séquences raccourcies doivent faire plus de 200 pb.

```{r}
(primer_log <- primer_trim(
  forward_files = fnFs,
  reverse_files = fnRs,
  primer_fwd = primer_fwd,
  primer_rev = primer_rev,
  output_dir = path_to_trimmed_reads,
  min_size = 200
))
```

On extrait les séquences raccourcies pour les séquences directes et complémentaires en demandant tous les noms.

```{r}
nopFw <- sort(list.files(path_to_trimmed_reads, pattern ="R1", full.names = TRUE))
nopRv <- sort(list.files(path_to_trimmed_reads, pattern ="R2", full.names = TRUE))
```

5.

On crée un dossier menant aux séquences filtrées ainsi que les variables qui serviront ensuite.

```{r}
path_to_filtered_reads <- here::here("outputs", "dada2", "filtered")
if (!dir.exists(path_to_filtered_reads)) dir.create(path_to_filtered_reads, recursive = TRUE)
```

```{r}
filtFs <- file.path(path_to_filtered_reads, basename(fnFs))
filtRs <- file.path(path_to_filtered_reads, basename(fnRs))
```

```{r}
names(filtFs) <- sample_names
names(filtRs) <- sample_names
```

On extrait uniquement les séquences sélectionnées par dada2. On entre les variables correspondant aux séquences filtrées raccourcies directes et indecteres. Dada2 ne prend pas en compte les séquences plus petites que 150nt. Il faut que chaque R1 ait une séquence R2 associée. Il ne faut pas d'ambiguités (argument maxN). MaxEE donne le nombre d'erreurs maximales admises, basée sur le calcul du Qscore.
truncQ retire les séquences inférieure à un Qscore donné (2 = QScore de 20)
```{r}
(out <- dada2::filterAndTrim(
  fwd = nopFw,
  filt = filtFs,
  rev = nopRv,
  filt.rev = filtRs,
  minLen = 150,
  matchIDs = TRUE,
  maxN = 0,
  maxEE = c(3, 3),
  truncQ = 2
))
```

6.

Le taux d'erreur est estimé ensuite. On attribue à un nouvel objet errF et errR la fonction d'estimation d'erreurs de dada2, avec comme argument les séquences directes filtrées tirées aléatoirement parmi toutes les séquences filtrées totales. On applique la même fonction pour les séquences reverses. On applique ensuite la fonction dada2::plotErrors pour faire un graphique des erreurs estimées de chaque transition de base.

```{r}
errF <- dada2::learnErrors(filtFs,
                           randomize = TRUE,
                           multithread = TRUE)

errR <- dada2::learnErrors(filtRs,
                           randomize = TRUE,
                           multithread = TRUE)
dada2::plotErrors(errF, nominalQ=TRUE)
```

On retire ensuite les séquences lues en double. Pour chaque séquence unique, on compte le nombre de lectures.

```{r}
derepFs <- dada2::derepFastq(filtFs, verbose = TRUE)
derepRs <- dada2::derepFastq(filtRs, verbose = TRUE)
```

Ensuite, on fait correspondre l'erreur estimée sur les séquences uniques. 

```{r}
dadaFs <- dada2::dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada2::dada(derepRs, err = errR, multithread = TRUE)
```

7. Cette fonction permet d'associer les séquences entre elles sans mismatch (erreur d'association des bases). L'argument verbose permet d'avoir un résumer de la fonction.


```{r}
mergers <- dada2::mergePairs(
  dadaF = dadaFs,
  derepF = derepFs,
  dadaR = dadaRs,
  derepR = derepRs,
  maxMismatch = 0,
  verbose = TRUE
)
```

8.

On fabrique une table compilant les séquences associées précedemment, avec le nombre de lecture pour chaque séquence.

```{r}
seqtab <- dada2::makeSequenceTable(mergers)
```

9.

On retire les séquences chimères qui résultent d'un mauvais appariement. Chaque séquence est passée au crible. On a ensuite le résumé.

```{r}
seqtab_nochim <- dada2::removeBimeraDenovo(seqtab,
                                           method = "consensus",
                                           multithread = TRUE,
                                           verbose = TRUE)
```

10.

On associe ensuite une appartenance taxonomique à nos séquences. Cela permettra l'interprétation de nos séquences dans l'environnement. Cette appartenance s'effectue en 2 étapes. D'abord, on utilise l'algorithme de Wang et al pour assigner la taxonomie. On utilise les séquences précedemment obtenues. refFasta donne le chemin vers les séquences.taxLevels attribue le niveau taxonomique. minBoot est le niveau de confiance minimum pour l'aassociation taxonimique.

```{r}
taxonomy <- dada2::assignTaxonomy(
  seqs = seqtab_nochim,
  refFasta = silva_train_set,
  taxLevels = c("Kingdom", "Phylum", "Class",
                "Order", "Family", "Genus",
                "Species"),
  multithread = TRUE,
  minBoot = 60
)
```

Puis on attribue le rang espèces aux ASV qui sont identique à une séquence de référence.

```{r}
taxonomy <- dada2::addSpecies(
  taxonomy,
  silva_species_assignment,
  allowMultiple = FALSE
)
```

11.

On exporte les données sous forme d'objets R, un pour la table d'ASV et une autre pour la taxonomie. On crée le fichier et on indique le chemin à R.

```{r}
export_folder <- here::here("outputs", "dada2", "asv_table")

if (!dir.exists(export_folder)) dir.create(export_folder, recursive = TRUE)

saveRDS(object = seqtab_nochim,
        file = file.path(export_folder, "seqtab_nochim.rds"))

saveRDS(object = taxonomy,
        file = file.path(export_folder, "taxonomy.rds"))
```


On collecte les séquences ASV et on attribue des ID uniques pour chaque séquence, afin que leur nom soit plus court. On renomme les variables en leur attribuant ce nouveau nom.

```{r}
asv_seq <- colnames(seqtab_nochim)
ndigits <- nchar(length(asv_seq))
asv_id <- sprintf(paste0("ASV_%0", ndigits, "d"), seq_along(asv_seq))
row.names(taxonomy) <- colnames(seqtab_nochim) <- names(asv_seq) <- asv_id
```

Ces nouvelles identités sont collectées dans une nouvelle colonne asv. 

```{r}
taxonomy_export <- df_export(taxonomy, new_rn = "asv")

seqtab_nochim_export <- t(seqtab_nochim)
seqtab_nochim_export <- df_export(seqtab_nochim_export, new_rn = "asv")
```

On exporte ensuite la taxonomie,

```{r}
write.table(taxonomy_export,
            file = file.path(export_folder, "taxonomy.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

puis la table ASV,

```{r}
write.table(seqtab_nochim_export,
            file = file.path(export_folder, "asv_table.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

et les séquences en format FASTA

```{r}
cat(paste0(">", names(asv_seq), "\n", asv_seq),
    sep = "\n",
    file = file.path(export_folder, "asv.fasta"))
```

On assemble la table regroupant les statistiques à propos de chaque étape vu ci-dessus.

```{r}
getN <- function(x) sum(dada2::getUniques(x))

log_table <- data.frame(
  input = primer_log$in_reads,
  with_fwd_primer = primer_log$`w/adapters`,
  with_rev_primer = primer_log$`w/adapters2` ,
  with_both_primers = out[, 1],
  filtered = out[, 2],
  denoisedF = sapply(dadaFs, getN),
  denoisedR = sapply(dadaRs, getN),
  merged = sapply(mergers, getN),
  nonchim = rowSums(seqtab_nochim),
  perc_retained = rowSums(seqtab_nochim) / out[, 1] * 100
)

rownames(log_table) <- sample_names
```

On exporte ensuite cette table.

```{r}
df_export(log_table, new_rn = "sample") |>
  write.table(file = file.path(export_folder, "log_table.tsv"),
              quote = FALSE,
              sep = "\t",
              row.names = FALSE)
```
